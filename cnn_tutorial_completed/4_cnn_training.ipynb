{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1Dnz_lioVll"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxyGHHRVobFc"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/CNN_tutorial/imagenette2-160.zip'\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('./data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXQJQG68nTAf"
      },
      "source": [
        "### Section 4: Training CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4sO7fOenTAh"
      },
      "source": [
        "#### 4.1 Getting our dataset\n",
        "We will be using [Imagenette](https://github.com/fastai/imagenette) as our classification dataset. This is a 10-class subset of the popular ImageNet dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3TYbqepnTAi"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E35Qlh77nTAj"
      },
      "outputs": [],
      "source": [
        "train_data = torchvision.datasets.Imagenette(root='./data', size='160px',  split='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkyPXrAxnTAk"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSPb51XqnTAk"
      },
      "outputs": [],
      "source": [
        "class_labels = {\n",
        "    0: \"tench\",\n",
        "    1: \"english springer\",\n",
        "    2: \"casette player\",\n",
        "    3: \"chain saw\",\n",
        "    4: \"church\",\n",
        "    5: \"french horn\",\n",
        "    6: \"garbage truck\",\n",
        "    7: \"gas pump\",\n",
        "    8: \"golf ball\",\n",
        "    9: \"parachute\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIgxLbiPLn7o"
      },
      "outputs": [],
      "source": [
        "# visualising sample images and counting images per class\n",
        "def visualise_dataset(ds, tensor=False):\n",
        "    imgs_to_visualize = {}\n",
        "    class_count = {}\n",
        "\n",
        "    for img, label in ds:\n",
        "        # create a label key and add one image plus start count\n",
        "        if label not in imgs_to_visualize.keys():\n",
        "            imgs_to_visualize[label] = img\n",
        "            class_count[label] = 1\n",
        "\n",
        "        # keep counting for each new label\n",
        "        else:\n",
        "            class_count[label] += 1\n",
        "\n",
        "    fig, axs = plt.subplots(2, 5, figsize=(10, 6)) # empty canvas with 10 subplots (2 rows, 5 cols)\n",
        "    axs = axs.flatten() # since axes are 2D, we can flatten them to easily access\n",
        "\n",
        "    for i, (label_name, img) in enumerate(imgs_to_visualize.items()):\n",
        "        if tensor:\n",
        "            img = img.permute(1, 2, 0)\n",
        "\n",
        "        ax = axs[i] #select axes\n",
        "        ax.imshow(img)  #show img\n",
        "        ax.set_title(f\"{class_labels[label_name]} ({class_count[label_name]})\", fontsize=10) #add class name and class count as title\n",
        "        ax.axis('off')\n",
        "\n",
        "\n",
        "    fig.suptitle(\"Sample images from Imagenette\", fontsize=18)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualise_dataset(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9vLv5bonTAl"
      },
      "source": [
        "#### 4.2 Getting our data ready for the CNN\n",
        "\n",
        "Problems with our dataset currently:\n",
        "\n",
        "- each image is stored as an Image object\n",
        "- each image is a rectangle of non-uniform size\n",
        "\n",
        "We can transform our dataset to solve these"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH_Y_vGlnTAl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# define a composition of transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Convert from Image to Tensor\n",
        "    transforms.Resize((160, 160)), # Resize to square (160x160)px\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # ImageNet Channelwise Mean and SD\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxNsoIkrnTAl"
      },
      "outputs": [],
      "source": [
        "# apply these transformations to the dataset\n",
        "train_ds = torchvision.datasets.Imagenette(root='./data', size='160px',  split='train', transform=transform)\n",
        "test_ds = torchvision.datasets.Imagenette(root='./data', size='160px',  split='val', transform=transform)\n",
        "\n",
        "# create data loaders (optimized for CNN access - batched loading into memory, parallelization, shuffling, etc)\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBogk79m8S9t"
      },
      "outputs": [],
      "source": [
        "visualise_dataset(train_ds, tensor=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V2VihJanTAm"
      },
      "source": [
        "#### 4.3 Building our CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzEtXTl5-75U"
      },
      "source": [
        "CNN Architecture\n",
        "\n",
        "![Image0](https://www.researchgate.net/publication/336805909/figure/fig1/AS:817888827023360@1572011300751/Schematic-diagram-of-a-basic-convolutional-neural-network-CNN-architecture-26.ppm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edtwwjuW9olQ"
      },
      "source": [
        "Convolutional Layer\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize:fit:1400/1*L1SVH2rBxGvJx3L4aB59Cg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZeFVzkX-0B2"
      },
      "source": [
        "Pooling Layer\n",
        "\n",
        "![Image2](https://pyimagesearch.com/wp-content/uploads/2021/05/Convolutional-Neural-Networks-CNNs-and-Layer-Types.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYLemPrypv5K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kge6BBI8p0WA"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(128 * 20 * 20, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv Block 2\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv Block 3\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.reshape(x.size(0), -1)  # Flatten for the fully connected layer\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc1(x) # Representation Layer\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc2(x) # Final Classification Layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSWC6Y8up4tb"
      },
      "outputs": [],
      "source": [
        "simple_model = SimpleCNN()\n",
        "loss_fn = nn.CrossEntropyLoss() # loss function\n",
        "optimizer = optim.SGD(simple_model.parameters()) # optimizer that will update the weights of our CNN\n",
        "num_training_epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT61YGlHroJT"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "simple_model.to(device) # move model to gpu\n",
        "print(f\"Using device {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfD95i4Nq0l4"
      },
      "source": [
        "#### Training our Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT2S34e3pz37"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_loop(model, epochs, train_dataloader):\n",
        "  accuracies = []\n",
        "  losses = []\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    for imgs, labels in tqdm(train_dataloader):\n",
        "      # forward pass\n",
        "      imgs, labels = imgs.to(device), labels.to(device)\n",
        "      optimizer.zero_grad() #reset the gradients\n",
        "      output = model(imgs)\n",
        "      loss = loss_fn(output, labels)\n",
        "\n",
        "      # backward pass\n",
        "      loss.backward() #gradients are computed for each parameter/weight\n",
        "      optimizer.step()  #update the parameters with SGD\n",
        "\n",
        "      running_loss += loss.item() # keep track of batch loss\n",
        "      _, preds = torch.max(output, 1) # find the max indices (preds)\n",
        "      running_acc += torch.sum(preds == labels) #keep track of batch accuracy\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataloader) #loss is computed per batch\n",
        "    epoch_acc = running_acc / len(train_dataloader.dataset) #acc is computed per sample\n",
        "\n",
        "    print(f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}\")\n",
        "    losses.append(epoch_loss)\n",
        "    accuracies.append(epoch_acc.item())\n",
        "\n",
        "  return losses, accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utq8_w8NLn7q"
      },
      "outputs": [],
      "source": [
        "losses, accuracies = train_loop(simple_model, 5, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBhdVT2U4tab"
      },
      "outputs": [],
      "source": [
        "def plot_train_loss_accuracies(loss, accuracies):\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax[0].plot(loss)\n",
        "    ax[0].set_title('Train Loss')\n",
        "    ax[0].set_xlabel('Epochs')\n",
        "\n",
        "    ax[1].plot(accuracies)\n",
        "    ax[1].set_title('Train Accuracy')\n",
        "    ax[1].set_ylim(0, 1)\n",
        "    ax[1].set_xlim(0, num_training_epochs)\n",
        "    ax[1].set_xlabel('Epochs')\n",
        "\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "plot_train_loss_accuracies(losses, accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PjNy1PK4SVq"
      },
      "outputs": [],
      "source": [
        "def get_test_accuracy(model, test_dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28OmhxzL4a3e"
      },
      "outputs": [],
      "source": [
        "get_test_accuracy(simple_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bujrWLrGs9Iq"
      },
      "source": [
        "#### Go deeper!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tji8U1m5s83b"
      },
      "outputs": [],
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer12 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer13 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(5*5*512, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer9(out)\n",
        "        out = self.layer10(out)\n",
        "        out = self.layer11(out)\n",
        "        out = self.layer12(out)\n",
        "        out = self.layer13(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pey5HsLBtoSl"
      },
      "outputs": [],
      "source": [
        "vgg_model = VGG16()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(vgg_model.parameters())\n",
        "num_training_epochs = 5\n",
        "vgg_model = vgg_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJdYi0uLt5Fy"
      },
      "outputs": [],
      "source": [
        "vgg_losses, vgg_accuracies = train_loop(vgg_model, 5, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO1vVgoFLn7r"
      },
      "outputs": [],
      "source": [
        "model_state_dict = torch.load('/content/drive/MyDrive/CNN_tutorial/vgg_model_e20.pth')\n",
        "better_vgg = VGG16()\n",
        "better_vgg.load_state_dict(model_state_dict)\n",
        "better_vgg = better_vgg.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts3rrBlmLn7s"
      },
      "outputs": [],
      "source": [
        "get_test_accuracy(better_vgg, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8KUHdPw2oe4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_confusion_matrix(model, test_loader, class_names):\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # creates a confusion matrix using scikit-learn\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # plotting the confusion matrix as a heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx5vo4MMLn7s"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(simple_model, test_loader, class_labels.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0H6wMyQLn7s"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(better_vgg, test_loader, class_labels.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tisoDmDt_0Id"
      },
      "outputs": [],
      "source": [
        "def visualize_model_pred(model, index):\n",
        "    img, label = train_data[index]\n",
        "    img_model, label_model = train_ds[index]\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(img)\n",
        "\n",
        "    pred_label = model(img_model.unsqueeze(0).to(device))\n",
        "    _, pred = torch.max(pred_label, 1)\n",
        "\n",
        "    true_label = class_labels[label_model]\n",
        "    pred_label = class_labels[pred.item()]\n",
        "    plt.title(f\"True: {true_label},  Pred:{pred_label}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQfLrm9KLn7s"
      },
      "outputs": [],
      "source": [
        "# visualize_model_pred(simple_model, 100)\n",
        "visualize_model_pred(better_vgg, 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-qU6eJhLn7s"
      },
      "source": [
        "### 4.4 Finetuning your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr7qTflBLn7w"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "ft_ds_path = '/content/drive/MyDrive/CNN_tutorial/hymenoptera_data.zip'\n",
        "\n",
        "with zipfile.ZipFile(ft_ds_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('./data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re25CPNXLn7w"
      },
      "outputs": [],
      "source": [
        "input_size = 224\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        # transforms.RandomResizedCrop(input_size), # Augmentation 1\n",
        "        # transforms.RandomHorizontalFlip(),        # Augmentation 2\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp-RiENZLn7w"
      },
      "outputs": [],
      "source": [
        "ft_train_ds = torchvision.datasets.ImageFolder(root='./data/hymenoptera_data/train', transform=data_transforms['train'])\n",
        "ft_test_ds = torchvision.datasets.ImageFolder(root='./data/hymenoptera_data/val', transform=data_transforms['val'])\n",
        "\n",
        "\n",
        "ft_train_loader = torch.utils.data.DataLoader(ft_train_ds, batch_size=32, shuffle=True)\n",
        "ft_test_loader = torch.utils.data.DataLoader(ft_test_ds, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fb7st92Ln7w"
      },
      "outputs": [],
      "source": [
        "imgs_to_visualize = {}\n",
        "class_count = {}\n",
        "\n",
        "ft_class_labels = {\n",
        "    0: \"ants\",\n",
        "    1: \"bees\"\n",
        "}\n",
        "\n",
        "for img, label in ft_train_ds:\n",
        "    if label not in imgs_to_visualize.keys():\n",
        "        imgs_to_visualize[label] = img\n",
        "        class_count[label] = 1\n",
        "    else:\n",
        "        class_count[label] += 1\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axs = axs.flatten()\n",
        "\n",
        "for i, (label_name, img) in enumerate(imgs_to_visualize.items()):\n",
        "    ax = axs[i]\n",
        "    img = img.permute(1, 2, 0)\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f\"{ft_class_labels[label_name]} ({class_count[label_name]})\")\n",
        "    ax.axis('off')\n",
        "\n",
        "\n",
        "fig.suptitle(\"Sample images\", fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBDFYKesLn7w"
      },
      "outputs": [],
      "source": [
        "ft_vgg = torchvision.models.vgg16(weights='DEFAULT') # downloading a model from torchvision pretrained on ImageNet\n",
        "\n",
        "# freezing all the weights\n",
        "for param in ft_vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "ft_vgg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbCzrQ7gLn7x"
      },
      "outputs": [],
      "source": [
        "# replace final classification layer\n",
        "num_features = ft_vgg.classifier[6].in_features\n",
        "ft_vgg.classifier[6] = nn.Linear(num_features, 2) #our dataset has 2 classes\n",
        "ft_vgg.to(device)\n",
        "\n",
        "# check which layers are trainable (should only be the final layer \"classifier.6\")\n",
        "params_to_update = []\n",
        "for name,param in ft_vgg.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name, param.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-MdFfzFLn7x"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(params_to_update, lr=0.001) #only feeding in the \"params_to_update\" to the optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "ft_loss, ft_accuracy = train_loop(ft_vgg, 5, ft_train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXEb_dn6Ln7x"
      },
      "outputs": [],
      "source": [
        "vgg_random = torchvision.models.vgg16(weights=None)\n",
        "\n",
        "num_features = vgg_random.classifier[6].in_features\n",
        "vgg_random.classifier[6] = nn.Linear(num_features, 2) #our dataset has 2 classes\n",
        "vgg_random.to(device)\n",
        "\n",
        "optimizer = optim.SGD(vgg_random.parameters(), lr=0.001)\n",
        "\n",
        "rand_loss, rand_accuracy = train_loop(vgg_random, 5, ft_train_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snbfvVsJLn7x"
      },
      "source": [
        "### 4.5 Extracting Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edxY2uKdLn7x"
      },
      "outputs": [],
      "source": [
        "ft_vgg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCtOtrXkLn7x"
      },
      "outputs": [],
      "source": [
        "activations = {}\n",
        "# function that will get activation from layer with name \"name\"\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activations[name] = np.squeeze(output.cpu().detach()) #squeeze-removes batchdim, cpu-moves to cpu, detach-removes gradient\n",
        "    return hook\n",
        "\n",
        "ft_vgg.classifier[3].register_forward_hook(get_activation('classifier.linear')) #register_forward_hook requries a \"hook\" sub function which it calls internally\n",
        "\n",
        "# feeding a random input, could be an image tensor\n",
        "X = torch.rand(1, 3, 224, 224).to(device)\n",
        "\n",
        "output = ft_vgg(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQLDsv69Ln7x"
      },
      "outputs": [],
      "source": [
        "activations['classifier.linear']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cUkV76FLn7x"
      },
      "outputs": [],
      "source": [
        "import scipy.io as sio\n",
        "\n",
        "mat_data = {'activations': activations['classifier.linear'].numpy()}\n",
        "sio.savemat('activations.mat', mat_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww1u2UwzLn7x"
      },
      "source": [
        "### 4.5 Moving a model to MATLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U05q19UzLn7y"
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.resnet18(pretrained=True) # tracing only works with 'modern' networks\n",
        "model.eval()\n",
        "model.to(\"cpu\")\n",
        "X = torch.rand(1, 3, 224, 224) # example input required to trace\n",
        "traced_model = torch.jit.trace(model.forward, X) #tracing the model and saving it in a 'Script' format that is Python-free\n",
        "traced_model.save('traced_model.pt')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}